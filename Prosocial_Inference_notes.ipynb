{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Minimal code for inverting a utility function\n",
    "import numpy as np\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A forward model (i.e. a mental model of how someone makes choices) to infer someone's mental states \n",
    "#(in this case, their reward function) from their choice. I added a lot of comments and wrote different \n",
    "#versions of the same function so it's easier to go through. \n",
    "\n",
    "#A niceness model would be more complicated:\n",
    "#Each choice associated with a list of rewards (one for each event dimension that the observer cares about)\n",
    "#Model would add up rewards associated with a choice to calculate final utility\n",
    "\n",
    "    #A gives a to B -> A sacrifices\n",
    "    #A gives bb to B -> A sacrifices + makes the agent happy\n",
    "\n",
    "#If an observer cares about sacrifice but also about making the other person happy, then all choices \n",
    "#would have a sacrifice reward, but only the choice that the other agent likes would also have a \"making \n",
    "#the other person happy\" reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JULIAN'S CODE ###\n",
    "\n",
    "def ToM(Desires, Tau=0.01):\n",
    "\t\"\"\"\n",
    "\tMinimal mental model.\n",
    "\tThis model does not take beliefs into account and only assumes that the agent is more likely to choose higher utilities.\n",
    "\tArguments:\n",
    "\tDesires [list]: an agent's desires formalized as a reward function\n",
    "\tTau [float]: agent's level of rationality. As Tau is closer to 0, the agent is optimal, as Tau goes to infinity the agent selects randomly.\n",
    "\tBy default Tau is set to assume that the agent is rational.\n",
    "\t\"\"\"\n",
    "\t# Transform the desires into probabilities of acting:\n",
    "\t# First, create a probability vector that equals the number of desires that the agent has\n",
    "\tProbabilities = [0] * len(Desires)\n",
    "\t# Iterate over each probability, and compute the soft-maxed desire\n",
    "\tfor i in range(len(Probabilities)):\n",
    "\t\tProbabilities[i] = np.exp(Desires[i]/Tau)\n",
    "\t# Softmaxed vector is not a probability distribution because it does not add up to 1, so normalize it.\n",
    "\tNormalizingConstant = sum(Probabilities)\n",
    "\tfor i in range(len(Probabilities)):\n",
    "\t\tProbabilities[i] = Probabilities[i]/NormalizingConstant\n",
    "\t# Now you have a vector of probabilities. Make a choice.\n",
    "\tChoiceSpace = list(range(len(Desires))) # Just create a list from 0 to n.\n",
    "\treturn(np.random.choice(ChoiceSpace,p=Probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# #ChoiceSpace = list(range(len(Desires)))\n",
    "\n",
    "# #ChoiceSpace\n",
    "# \tDesires = [10,0,2,1]\n",
    "# \tProbabilities = [0] * len(Desires)\n",
    "# for i in range(len(Probabilities)):\n",
    "# \t\tProbabilities[i] = np.exp(Desires[i]/Tau)\n",
    "# \t# Softmaxed vector is not a probability distribution because it does not add up to 1, so normalize it.\n",
    "# \tNormalizingConstant = sum(Probabilities)\n",
    "# \tfor i in range(len(Probabilities)):\n",
    "# \t\tProbabilities[i] = Probabilities[i]/NormalizingConstant\n",
    "# \treturn(Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToM_short(Desires, Tau=0.01):\n",
    "\t\"\"\"\n",
    "\tEquivalent function as above, but compressed using list comprehension.\n",
    "\tThis function includes a simple math trick that doesn't affect the results\n",
    "\tbut prevents underflow and overflow from happening.\n",
    "\t\"\"\"\n",
    "\tC = max(Desires) # This is a simple trick that doesn't affect the result after softmaxing\n",
    "\t# but it prevents numerical underflow (numbers that are too small) and overflow (numbers that are too big)\n",
    "\tProbabilities = [np.exp(i-C)/Tau for i in Desires]\n",
    "\tProbabilities = [i/sum(Probabilities) for i in Probabilities]\n",
    "\treturn(np.random.choice(list(range(len(Desires))),p=Probabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are probabilistic functions, but the noise depends on rationality. For instance:\n",
    "ToM([10,0]) # Agent has two options, rewards are 10 and 0, and (by default) agent is pretty rational. Should always select choose 10 [the choice in position 0 [0,1]] no matter how many times you run it.\n",
    "[ToM([10,0]) for i in range(10)] # Run this agent 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 1, 0, 1, 1, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToM([10,0],Tau=500) # Lesion rationality. This agent should start choosing the option in position 1 of [0,1] fairly often.\n",
    "[ToM([10,0], Tau=500) for i in range(10)] # Run irrational agent 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When the utilities get big, simple softmax starts to break:\n",
    "ToM([99,100]) # Should give the wrong answers: 0 instead of 1.\n",
    "[ToM([99,100]) for i in range(10)] # Consistently chooses option 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is because the exponential of the utility (the desire divided by Tau) is larger than what can be stored in a floating point variable.\n",
    "# ToM_short fixes that:\n",
    "ToM_short([99,100])\n",
    "# Should now choose a mixture between 0 and 1 since they're both pretty much identical.\n",
    "[ToM_short([99,100]) for i in range(10)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strengthening rationality a lot. Agent should now choose option 1 more often.\n",
    "[ToM_short([99,100], Tau=0.000001) for i in range(10)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DESIRE INFERENCE MODEL\n",
    "# Now we need a model for an observer who sees a choice and tries to infer the underlying reward function.\n",
    "# To do this, we need an edited model of the agent, where instead of returning an action, it returns the probability of a given action.\n",
    "def ToM_prob(Desires, Choice, Tau=0.01):\n",
    "\t\"\"\"\n",
    "\tSame model as above, but this function does not return a choice based on the desires, instead, it returns the probability of a choice.\n",
    "\t\"\"\"\n",
    "\tC = max(Desires)\n",
    "\tProbabilities = [np.exp((i-C)/Tau) for i in Desires]\n",
    "\tProbabilities = [i/sum(Probabilities) for i in Probabilities]\n",
    "\t# Everything is identical so far. But now, instead of sampling an action based on the probabilities,\n",
    "\t# just return the probability of the action of the choice\n",
    "\treturn(Probabilities[Choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferDesires(Options, Choice, Tau=0.01):\n",
    "\t\"\"\"\n",
    "\tGets the number of options that an agent can choose from, gets the choice that the agent made, and the agent's rationality.\n",
    "\tFunction uses Bayesian inference to infer the agent's desires, given their choice.\n",
    "\t\"\"\"\n",
    "\t# In a niceness model; Each choice associated w/ a list of rewards (one for each factor observer weights)\n",
    "\t# Model would add up rewards associated with a choice to calculate final utility\n",
    "\t## Inference Example Model\n",
    "\t# Easiest thing to do is just Monte-Carlo sampling.\n",
    "\tSamples = 500\n",
    "\t# Generate Monte Carlo samples:\n",
    "\tMC_Samples = [list(np.random.randint(0,50,Options)) for i in range(Samples)]\n",
    "\t# MC_Samples is now a list of samples. Each sample is a list (so this is a list of lists) with one reward between 0 and 50 for each option.\n",
    "\t# Biases go here. For now, Just assume any reward function is as probable.\n",
    "\tPrior = [1.0/Samples] * Samples\n",
    "\t# Compute the likelihood of each sample:\n",
    "\t# This is just calling ToM_prob, always with the same observed choice (Choice) and the same rationality (Tau) which we got as input\n",
    "\t# And it's calculating how likely different reward functions are.\n",
    "\tLikelihood = [ToM_prob(MC_Samples[i], Choice, Tau) for i in range(Samples)]\n",
    "\t# Now compute posterior\n",
    "\tPosterior = [Prior[i]*Likelihood[i] for i in range(Samples)]\n",
    "\t# Normalize\n",
    "\tPosterior = [i/sum(Posterior) for i in Posterior]\n",
    "\t# Marginalize over the distribution to get the expected reward function:\n",
    "\tInferredRewards = [0] * Options\n",
    "\t# This is just applying the marginalization rule to the monte carlo samples, but let me know if it's confusing.\n",
    "\tfor i in range(Options):\n",
    "\t\tInferredRewards[i] = sum([MC_Samples[x][i]*Posterior[x] for x in range(Samples)])\n",
    "\treturn(InferredRewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples = 500\n",
    "# MC_Samples = [list(np.random.randint(0,50,2)) for i in range(Samples)]\n",
    "# MC_Samples #we perhaps might bias these with an expected reward function\n",
    "#Prior = [1.0/Samples] * Samples\n",
    "#Prior #All options equally likely\n",
    "# Likelihood = [ToM_prob(MC_Samples[i], 0, 0.01) for i in range(Samples)]\n",
    "# Likelihood \n",
    "#InferredRewards = [0] * 3 # with 3 options, creates an event space of N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31.69246861924678, 15.64225941422591]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at inferences:\n",
    "InferDesires(2,0) # two options and agent chooses option 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.608038674774853, 24.322515819852608]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferDesires(2,0,Tau=1000) # should infer weaker preference because agent is less rational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.904999999999987,\n",
       " 18.92999999999998,\n",
       " 22.45999999999998,\n",
       " 41.334999999999965,\n",
       " 19.92499999999999]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferDesires(5,3, Tau=.001) # Should have a more or less uniform vector, with a higher reward on choice 3 [0,1,2,3,4] (fourth position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19.097046413502106,\n",
       " 18.683544303797454,\n",
       " 20.510548523206747,\n",
       " 38.464135021097015]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InferDesires(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#### PROSOCIAL INFERENCE ####\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add features to the social environment ('Event')\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, options, agentvalue, agentbeliefs, recipientvalue, choicespace, choice, outcome): # 'self' allows function to call itself\n",
    "        self.options = options # unique resources a prosocial agent can share \n",
    "        self.agentvalue = agentvalue # the value the prosocial agent places on the resources being shared\n",
    "        self.agentbeliefs = agentbeliefs # agents beliefs (certainty) about what the recipient values\n",
    "        self.recipientvalue = recipientvalue # the value the recipient places on the resources (i.e., their needs)\n",
    "        self.choicespace = choicespace # all possible prosocial choices\n",
    "        self.choice = choice # the prosocial agent's choice\n",
    "        self.outcome = outcome # prosocial agent's benefits from acting (e.g, praise, good feelings)\n",
    "        \n",
    "#additions\n",
    "    #social norms (action is normative (common) vs supra-normative)\n",
    "    #reward expectations (expects to benefit or does not)\n",
    "    #reward outcomes (benefits or does not)\n",
    "    #reactions (happy or not happy)\n",
    "    \n",
    "    \n",
    "    # ToM should be called within a model of morality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define features of our specific prosocial scenario\n",
    "\n",
    "nice = Event(['a','b'], # options (unique actions)\n",
    "                  [1,1], # agentvalue (agent's value on actions)\n",
    "                  [[1,0],[0,1],[.5,.5]], # agentbeliefs (agent's beliefs (certainty) about recipient's value for each action)\n",
    "                  [10,0], # recipientvalue (recipient's value on unique actions)\n",
    "                  [['a','a'], # choicespace (possible actions) \n",
    "                   ['b','b'],\n",
    "                   ['a','b'],\n",
    "                  ['a','a','a','a'],\n",
    "                  ['b','b','b','b'],\n",
    "                  ['a','b','a','b']],\n",
    "                  2, # choice in choicespace (action)\n",
    "             1 # outcomes of choice (benefitted? y[1]/n[0])\n",
    "            ) \n",
    "\n",
    "#GOALS\n",
    "# finish agent who knows others' rewards\n",
    "# loop through everything that can vary, nested for loops\n",
    "# get predictions for each\n",
    "# look in R\n",
    "# some dimensions matter, some are interesting\n",
    "\n",
    "\n",
    "# norms (simulating past observation of people, knowledge, \n",
    "#get norms by asking people who much they expect people to give from turk, input that into the \n",
    "#model\n",
    "#- moral judgments \n",
    "#what people tell you the norms are, plug in to the model\n",
    "#or set parameter, and try to fit that parameter to look like  \n",
    "#\n",
    "#try to figoure out the norm, based on niceeness judgment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "niceness(Sacrifice model): 0.5\n",
      "niceness(Other-utility model): 0.25\n",
      "niceness(Purity model): 1\n"
     ]
    }
   ],
   "source": [
    "### SACRIFICE MODEL ###\n",
    "\n",
    "    ## Do some basic division\n",
    "\n",
    "sacr_outcome = len(nice.choicespace[nice.choice]) \\\n",
    "/len(max(nice.choicespace,key=len))\n",
    "\n",
    "\n",
    "print(\"niceness(Sacrifice model):\", sacr_outcome)\n",
    "\n",
    "## Other possible models (sacrifice relative to norm, or observed actions when repeated observation possible)\n",
    "\n",
    "\n",
    "### OTHER-UTILITY MODEL ###\n",
    "\n",
    "    ## Dictionary to convert unique actions into recipient utilities\n",
    "\n",
    "utility = {\n",
    "    nice.options[0]: nice.recipientvalue[0], # action 1 -> recipient utility 1\n",
    "    nice.options[1]: nice.recipientvalue[1] # action 2 -> recipient utility 2\n",
    "}\n",
    "\n",
    "    ## Get choice utilities via list comprehension\n",
    "u = []\n",
    "for choice in range(len(nice.choicespace)):\n",
    "    u.append(sum([utility[m] for m in nice.choicespace[choice]]))\n",
    "    \n",
    "util_outcome= abs(u[nice.choice]/max(u)) #Choice utility divided by max possibile choice utility\n",
    "\n",
    "\n",
    "print(\"niceness(Other-utility model):\", util_outcome)\n",
    "\n",
    "\n",
    "### PURITY MODEL ###\n",
    "\n",
    "    ## If the agent acted nice without benefitting, act was pure\n",
    "if len(nice.choicespace[nice.choice]) > 0:\n",
    "    pure_outcome=((len(nice.choicespace[nice.choice])*1) - nice.outcome)\n",
    "else:\n",
    "    pure_outcome=0\n",
    "print(\"niceness(Purity model):\", pure_outcome)\n",
    "\n",
    "\n",
    "### HYBRID MODEL ###\n",
    "\n",
    "#need to decide weights\n",
    "\n",
    "\n",
    "### ToM MODEL ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ToM MODEL\n",
    "\n",
    "#define an agent, just an event\n",
    "    #how much agent values each object [5,0]\n",
    "    #how much agent value giving any object [1,1]\n",
    "    #caring about others = 0-1\n",
    "    \n",
    "#where giving [a,a] would mean =  x\n",
    "# they chose  choice 2 out of 3.\n",
    "# simulate agents and see which agent would be maximizing \n",
    "# giving two 2, means don't care about value of a, or they care alot o\n",
    "\n",
    "# how much they value objects and how much they care about person are unknown\n",
    "\n",
    "# value of giving all objects should be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__main__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-2920d2e19b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m\"__name__\"\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m__main__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mgiver_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__main__' is not defined"
     ]
    }
   ],
   "source": [
    "#Michael's ToM niceness model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Giver doesn't know what Receiver wants\n",
    " \n",
    "def receiver(rationality, receiver_rewards, giver_action):\n",
    "\n",
    "\tU = receiver_rewards + actions[giver_action]\n",
    "\n",
    "\treturn U\n",
    "\n",
    "def selfish_giver(rationality, giver_rewards):\n",
    "\t\n",
    "\tU = giver_rewards\n",
    "\taction_probabilities = softmax(U, rationality)\n",
    "\n",
    "\treturn action_probabilities\n",
    "\n",
    "def selfless_giver(rationality, giver_rewards, receiver_rewards=None):\n",
    "\n",
    "\tif type(receiver_rewards) != type(None):\n",
    "\t\tU = giver_rewards + (cooperation*receiver_rewards)\n",
    "\telse:\n",
    "\t\treceiver_rewards = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS))\n",
    "\t\tgiver_actions = np.random.choice()\n",
    "\t\tfor receiver_reward in receiver_rewards:\n",
    "\t\t\tfor giver_action in giver_actions:\n",
    "\t\t\t\tU = giver\n",
    "\t\n",
    "\taction_probabilities = softmax(U, rationality)\n",
    "\n",
    "\treturn action_probabilities\n",
    "\n",
    "\n",
    "if \"__name__\" == __main__:\n",
    "\tgiver_rewards = np.array([4, 4])\n",
    "\n",
    "\tobjects = [\"orange\", \"apple\"]\n",
    "\tactions = np.random.choice(objects, p=(0.5, 0.5), size=())\n",
    "\n",
    "\tactions = {\"a\": 10, \"b\": -10}\n",
    "\tactions.keys()\n",
    "\tactions.values()\n",
    "\tactions[\"b\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-c052341dba99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Michael's ToM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#Michael's ToM model\n",
    "\n",
    "from utils import * #version issue?\n",
    "\n",
    "import itertools as it\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def agent_no_ToM(rationality, agent_reward, enforcer_action):\n",
    "\t# Compute the utilities.\n",
    "\tagent_cost = convert_cost(enforcer_action) if GRIDWORLD == True else NATURAL_COST + enforcer_action\n",
    "\tU = agent_reward - agent_cost\n",
    "\n",
    "\t# Compute the action probabilities.\n",
    "\taction_probabilities = softmax(U, rationality)\n",
    "\n",
    "\treturn action_probabilities\n",
    "\n",
    "def agent_ToM(rationality, agent_reward, enforcer_action, cooperation, cache=False, plot=False):\n",
    "\t# Set up the likelihood space.\n",
    "\tspace = tuple([MAX_VALUE for action in np.arange(NUM_ACTIONS)])\n",
    "\tlikelihood = np.zeros(space)\n",
    "\t\n",
    "\t# Generate possible enforcer rewards.\n",
    "\tif SAMPLING == True:\n",
    "\t\tenforcer_rewards = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS))\n",
    "\telse:\n",
    "\t\tenforcer_rewards = np.array(list(it.product(np.arange(MAX_VALUE), repeat=NUM_ACTIONS)))\n",
    "\n",
    "\t# Compute the likelihood.\n",
    "\tif cache == True:\n",
    "\t\tlikelihood = retrieve_enforcer_no_ToM(rationality, enforcer_rewards, enforcer_action, likelihood)\n",
    "\telse:\n",
    "\t\tfor enforcer_reward in enforcer_rewards:\n",
    "\t\t\tenforcer_action_probabilities = enforcer(rationality, enforcer_reward, cache=True)\n",
    "\t\t\tlikelihood[tuple(enforcer_reward)] = enforcer_action_probabilities[tuple(enforcer_action)]\n",
    "\n",
    "\t# Normalize the likelihood to generate the posterior.\n",
    "\tlikelihood = likelihood.flatten()\n",
    "\tif sum(likelihood) == 0:\n",
    "\t\tposterior = likelihood.reshape(space)\n",
    "\telse:\n",
    "\t\tposterior = (likelihood/sum(likelihood)).reshape(space)\n",
    "\n",
    "\t# Plot the posterior.\n",
    "\tif plot == True:\n",
    "\t\tplt.figure()\n",
    "\t\tplt.title(\"ToM Agent with Rationality = \" + str(rationality))\n",
    "\t\tplt.ylabel(\"Enforcer Rewards for Action 0\")\n",
    "\t\tplt.xlabel(\"Enforcer Rewards for Action 1\")\n",
    "\t\tplt.pcolor(posterior)\n",
    "\n",
    "\t# Compute the utilities.\n",
    "\tsmart_agent_reward = agent_reward + cooperative_reward(enforcer_rewards, posterior, cooperation)\n",
    "\tsmart_agent_cost = convert_cost(enforcer_action) if GRIDWORLD == True else NATURAL_COST + enforcer_action\n",
    "\tU = smart_agent_reward - smart_agent_cost\n",
    "\n",
    "\t# Compute the action probabilities.\n",
    "\taction_probabilities = softmax(U, rationality)\n",
    "\n",
    "\treturn action_probabilities\n",
    "\n",
    "def enforcer(rationality, enforcer_reward, p=0.0, cooperation=None, reward_assumptions=[], cache=False, plot=False):\n",
    "\t# Set up the utility space.\n",
    "\tspace = tuple([MAX_VALUE for action in np.arange(NUM_ACTIONS)]) if GRIDWORLD != True else \\\n",
    "\t\t\ttuple([GRIDWORLD_MAX_ACTION for action in np.arange(NUM_ACTIONS)])\n",
    "\tU = np.zeros(space)\n",
    "\t\n",
    "\t# Generate possible agent rewards and enforcer actions, taking into account\n",
    "\t# any potential assumptions the enforcer may have about agent rewards.\n",
    "\tif SAMPLING == True:\n",
    "\t\tagent_rewards = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS))\n",
    "\t\tenforcer_actions = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS)) if GRIDWORLD != True else \\\n",
    "\t\t\t\t\t\t   np.random.choice(GRIDWORLD_MAX_ACTION, (GRIDWORLD_MAX_SAMPLES, NUM_ACTIONS))\n",
    "\telse:\n",
    "\t\tif len(reward_assumptions) == 0:\n",
    "\t\t\tagent_rewards = np.array(list(it.product(np.arange(MAX_VALUE), repeat=NUM_ACTIONS)))\n",
    "\t\telif np.size(reward_assumptions) == NUM_ACTIONS:\n",
    "\t\t\tagent_rewards = np.array([reward_assumptions])\n",
    "\t\telse:\n",
    "\t\t\tagent_rewards = np.array(reward_assumptions)\n",
    "\t\tenforcer_actions = np.array(list(it.product(np.arange(MAX_VALUE), repeat=NUM_ACTIONS))) if GRIDWORLD != True else \\\n",
    "\t\t\t\t\t\t   np.array(list(it.product(np.arange(GRIDWORLD_MAX_ACTION), repeat=NUM_ACTIONS)))\n",
    "\n",
    "\t# Compute the utilities.\n",
    "\tif cache == True:\n",
    "\t\tU = retrieve_agent(rationality, enforcer_reward, agent_rewards, enforcer_actions, p, cooperation, U)\n",
    "\telse:\n",
    "\t\tU_agent_no_ToM = np.zeros(space)\n",
    "\t\tU_agent_ToM = np.zeros(space)\n",
    "\t\ttemp_agent_no_ToM = np.zeros(space)\n",
    "\t\ttemp_agent_ToM = np.zeros(space)\n",
    "\t\tfor agent_reward in agent_rewards:\n",
    "\t\t\tfor enforcer_action in enforcer_actions:\n",
    "\t\t\t\t# Reason about a non-ToM agent.\n",
    "\t\t\t\tif p != 1.0:\n",
    "\t\t\t\t\tagent_action_probabilities = agent_no_ToM(rationality, agent_reward, enforcer_action)\n",
    "\t\t\t\t\texpected_enforcer_reward = np.dot(enforcer_reward, agent_action_probabilities)\n",
    "\t\t\t\t\ttemp_agent_no_ToM[tuple(enforcer_action)] = expected_enforcer_reward - (COST_RATIO*sum(enforcer_action))\n",
    "\n",
    "\t\t\t\t# Reason about a ToM agent.\n",
    "\t\t\t\tif p != 0.0:\n",
    "\t\t\t\t\tagent_action_probabilities = agent_ToM(rationality, agent_reward, enforcer_action, cooperation, cache=True)\n",
    "\t\t\t\t\texpected_enforcer_reward = np.dot(enforcer_reward, agent_action_probabilities)\n",
    "\t\t\t\t\ttemp_agent_ToM[tuple(enforcer_action)] = expected_enforcer_reward - (COST_RATIO*sum(enforcer_action))\n",
    "\n",
    "\t\t\tU_agent_no_ToM = U_agent_no_ToM + temp_agent_no_ToM\n",
    "\t\t\tU_agent_ToM = U_agent_ToM + temp_agent_ToM\n",
    "\t\tU_agent_no_ToM = U_agent_no_ToM / len(agent_rewards)\n",
    "\t\tU_agent_ToM = U_agent_ToM / len(agent_rewards)\n",
    "\t\tU = ((1.0-p)*U_agent_no_ToM) + (p*U_agent_ToM)\n",
    "\n",
    "\t# Compute the action probabilities.\n",
    "\taction_probabilities = softmax(U.flatten(), rationality).reshape(space)\n",
    "\n",
    "\t# Plot the action probabilities.\n",
    "\tif plot == True:\n",
    "\t\tplt.figure()\n",
    "\t\tplt.title(\"Enforcing Agent with Rationality = \" + str(rationality))\n",
    "\t\tplt.ylabel(\"Agent Cost (Enforcer Action) for Action 0\")\n",
    "\t\tplt.xlabel(\"Agent Cost (Enforcer Action) for Action 1\")\n",
    "\t\tplt.pcolor(action_probabilities)\n",
    "\n",
    "\treturn action_probabilities\n",
    "\n",
    "def observer(infer, rationality, **kwargs):\n",
    "\t# Infer the enforcer's reward.\n",
    "\tif infer == \"enforcer_reward\":\n",
    "\t\t# Extract variables.\n",
    "\t\tcooperation = kwargs[\"cooperation\"]\n",
    "\t\tp = kwargs[\"p\"]\n",
    "\t\tenforcer_action = kwargs[\"enforcer_action\"]\n",
    "\t\tplot = kwargs[\"plot\"]\n",
    "\n",
    "\t\t# Set up the likelihood space.\n",
    "\t\tspace = tuple([MAX_VALUE for action in np.arange(NUM_ACTIONS)])\n",
    "\t\tlikelihood = np.zeros(space)\n",
    "\n",
    "\t\t# Generate possible enforcer rewards.\n",
    "\t\tif SAMPLING == True:\n",
    "\t\t\tenforcer_rewards = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS))\n",
    "\t\telse:\n",
    "\t\t\tenforcer_rewards = np.array(list(it.product(np.arange(MAX_VALUE), repeat=NUM_ACTIONS)))\n",
    "\t\t\n",
    "\t\t# Compute the likelihood.\n",
    "\t\tfor enforcer_reward in enforcer_rewards:\n",
    "\t\t\tenforcer_action_probabilities = enforcer(rationality, enforcer_reward, p=p, cooperation=cooperation, cache=True)\n",
    "\t\t\tlikelihood[tuple(enforcer_reward)] = enforcer_action_probabilities[tuple(enforcer_action)]\n",
    "\n",
    "\t\t# Normalize the likelihood to generate the posterior.\n",
    "\t\tlikelihood = likelihood.flatten()\n",
    "\t\tif sum(likelihood) == 0:\n",
    "\t\t\tposterior = likelihood.reshape(space)\n",
    "\t\telse:\n",
    "\t\t\tposterior = (likelihood/sum(likelihood)).reshape(space)\n",
    "\n",
    "\t\t# Plot the posterior.\n",
    "\t\tif plot == True:\n",
    "\t\t\tplt.figure()\n",
    "\t\t\tplt.title(\"Observer with Rationality = \" + str(rationality))\n",
    "\t\t\tplt.ylabel(\"Enforcer Rewards for Action 0\")\n",
    "\t\t\tplt.xlabel(\"Enforcer Rewards for Action 1\")\n",
    "\t\t\tplt.pcolor(posterior)\n",
    "\n",
    "\t# Infer the enforcer's beliefs about the cooperativeness of agents.\n",
    "\telif infer == \"cooperation\":\n",
    "\t\t# Extract variables.\n",
    "\t\tenforcer_reward = kwargs[\"enforcer_reward\"]\n",
    "\t\tp = kwargs[\"p\"]\n",
    "\t\tcooperation_set = kwargs[\"cooperation_set\"]\n",
    "\t\tenforcer_action = kwargs[\"enforcer_action\"]\n",
    "\t\tplot = kwargs[\"plot\"]\n",
    "\n",
    "\t\t# Set up the space of possible cooperation parameters and the\n",
    "\t\t# likelihood space.\n",
    "\t\tcooperation_set = np.array([-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 5.0])\n",
    "\t\tspace = np.shape(cooperation_set)\n",
    "\t\tlikelihood = np.zeros(space)\n",
    "\n",
    "\t\t# Compute the likelihood.\n",
    "\t\tfor c in np.arange(cooperation_set.size):\n",
    "\t\t\tenforcer_action_probabilities = enforcer(rationality, enforcer_reward, p=p, cooperation=cooperation_set[c], cache=True)\n",
    "\t\t\tlikelihood[c] = enforcer_action_probabilities[tuple(enforcer_action)]\n",
    "\n",
    "\t\t# Normalize the likelihood to generate the posterior.\n",
    "\t\tif sum(likelihood) == 0:\n",
    "\t\t\tposterior = likelihood\n",
    "\t\telse:\n",
    "\t\t\tposterior = likelihood / sum(likelihood)\n",
    "\t\t\n",
    "\t\t# Print the posterior.\n",
    "\t\tif plot == True:\n",
    "\t\t\tprint(posterior)\n",
    "\n",
    "\t# Infer the degree of ToM that the enforcer was acting for.\n",
    "\telif infer == \"p\":\n",
    "\t\t# Extract variables.\n",
    "\t\tenforcer_reward = kwargs[\"enforcer_reward\"]\n",
    "\t\tcooperation = kwargs[\"cooperation\"]\n",
    "\t\tenforcer_action = kwargs[\"enforcer_action\"]\n",
    "\t\tplot = kwargs[\"plot\"]\n",
    "\n",
    "\t\t# Set up the space of possible proportion parameters and the likelihood\n",
    "\t\t# space.\n",
    "\t\tp_set = np.linspace(0.0, 1.0, num=11)\n",
    "\t\tspace = np.shape(p_set)\n",
    "\t\tlikelihood = np.zeros(space)\n",
    "\n",
    "\t\t# Compute the likelihood.\n",
    "\t\tfor p in np.arange(p_set.size):\n",
    "\t\t\tenforcer_action_probabilities = enforcer(rationality, enforcer_reward, p=p_set[p], cooperation=cooperation, cache=True)\n",
    "\t\t\tlikelihood[p] = enforcer_action_probabilities[tuple(enforcer_action)]\n",
    "\n",
    "\t\t# Normalize the likelihood to generate the posterior.\n",
    "\t\tif sum(likelihood) == 0:\n",
    "\t\t\tposterior = likelihood\n",
    "\t\telse:\n",
    "\t\t\tposterior = likelihood / sum(likelihood)\n",
    "\n",
    "\t\t# Print the posterior.\n",
    "\t\tif plot == True:\n",
    "\t\t\tprint(posterior)\n",
    "\n",
    "\t# Jointly infer what the enforcer's beliefs of the agent rewards and\n",
    "\t# the degree of ToM that the enforcer was acting for.\n",
    "\telif infer == \"agent_reward_and_p\":\n",
    "\t\t# Extract variables.\n",
    "\t\tenforcer_reward = kwargs[\"enforcer_reward\"]\n",
    "\t\tcooperation = kwargs[\"cooperation\"]\n",
    "\t\tenforcer_action = kwargs[\"enforcer_action\"]\n",
    "\t\t\n",
    "\t\t# Set up the space of possible proportion parameters and the likelihood\n",
    "\t\t# space.\n",
    "\t\tp_set = np.linspace(0.0, 1.0, num=11)\n",
    "\t\tspace = (MAX_VALUE**2, p_set.size)\n",
    "\t\tlikelihood = np.zeros(space)\n",
    "\n",
    "\t\t# Generate possible enforcer rewards.\n",
    "\t\tif SAMPLING == True:\n",
    "\t\t\tagent_rewards = np.random.choice(MAX_VALUE, (MAX_SAMPLES, NUM_ACTIONS))\n",
    "\t\telse:\n",
    "\t\t\tagent_rewards = np.array(list(it.product(np.arange(MAX_VALUE), repeat=NUM_ACTIONS)))\n",
    "\n",
    "\t\t# Compute the likelihood.\n",
    "\t\tfor ar in np.arange(len(agent_rewards)):\n",
    "\t\t\tfor p in np.arange(p_set.size):\n",
    "\t\t\t\tenforcer_action_probabilities = enforcer(rationality, enforcer_reward, p=p_set[p], cooperation=cooperation, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t reward_assumptions=agent_rewards[ar], cache=True)\n",
    "\t\t\t\tlikelihood[ar][p] = enforcer_action_probabilities[tuple(enforcer_action)]\n",
    "\n",
    "\t\t# Normalize the likelihood to generate the posterior.\n",
    "\t\tlikelihood = likelihood.flatten()\n",
    "\t\tif sum(likelihood) == 0:\n",
    "\t\t\tposterior = likelihood.reshape(space)\n",
    "\t\telse:\n",
    "\t\t\tposterior = (likelihood/sum(likelihood)).reshape(space)\n",
    "\n",
    "\treturn posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Niceness(Options, Choice, Target_Util):\n",
    "    sacr_Outcome = []\n",
    "    oUtil_Outcome = []\n",
    "    ToM_Outcome = []\n",
    "    \n",
    "# sacrifice model [if enabled]: \n",
    "    sacr_outcome = len(Choice)/max(Choice,key=len) # elements in choice / total possible elements\n",
    "    # this model could be improved by sampling from largest amount observed across a set of observations\n",
    "    # norm model?\n",
    "            \n",
    "# other-utility model [if enabled]:\n",
    "    oUtil_Outcome = count(Target_Util) in Choice / max(Choice,key=len) #\n",
    "    \n",
    "# ToM model [if enabled]:\n",
    "    ToM_Outcome\n",
    "        InferDesires()\n",
    "    \n",
    "    Nice = sum[sacr_Outcome,oUtil_Outcome,ToM_Outcome] / count numeric values in [sacr_Outcome,oUtil_Outcome,ToM_Outcome]\n",
    "    return Nice\n",
    "    \n",
    "    \n",
    "\n",
    "#This model infers utility agents place on options based on choices\n",
    "#We want the utility observers place on which features of the choice\n",
    "#The model should add up rewards associated with a choice to calculate final utility\n",
    "\n",
    "#Agent chose 2 out of 0,1,2 -> Agent kinda prefers 2\n",
    "#Agent chose 2 out of 0=[a,a], 1=[b,b], 2=[a,a,a,a] -> Agent kinda prefers 2\n",
    "#Agent chose 2 out of 0=[.5], 1=[.5], 2=[1.0] -> Agent kinda prefers [sacrificing for others] -> Agent must be kinda nice\n",
    "\n",
    "#Agent in context A (not knowing target's preference) chose 2 \n",
    "\n",
    "##Event space:\n",
    "#(i) A thinks B wants = [a/b, ?]\n",
    "#(ii) A expects a reward = [0,1]\n",
    "#(iii) A gives = [a,a],[b,b],[a,b],[a,a,a,a],[b,b,b,b],[a,b,a,b]\n",
    "#(iv) B wants = [a] or [b]\n",
    "#(v) A gets a reward = [0,1]\n",
    "\n",
    "\n",
    "#Generosity model (count(iii)/max(iii))\n",
    "    #choice         ([a,a],[b,b],[a,b],[a,a,a,a],[b,b,b,b],[a,b,a,b])\n",
    "    #choice niceness ([.5],[.5],[.5],[1.0],[1.0],[1.0])\n",
    "    \n",
    "#Other-Utility model (sum contents of (iii) after multiplying by B's liking)/max(iii)\n",
    "    #if B likes a:\n",
    "    #choice         ([aa],[bb],[ab],[aaaa],[bbbb],[abab])\n",
    "    #choice niceness ([.5],[.0],[.25],[1.0],[0],[.5])\n",
    "    \n",
    "#ToM model (iii) defined by (i) where (i) can be a, b, or ?\n",
    "    #if A (mistakenly) thinks B likes b\n",
    "    #choice         ([aa],[bb],[ab],[aaaa],[bbbb],[abab])\n",
    "    #choice niceness ([.0],[.1],[.5],[0],[1],[.5])\n",
    "    \n",
    "    #if A is uncertain what B likes \n",
    "    #choice         ([aa],[bb],[ab],[aaaa],[bbbb],[abab])\n",
    "    #choice niceness ([.5],[.5],[1.0],[.5],[.5],[1.0])\n",
    "    \n",
    "\n",
    "# In this model \"Choice\" = sum of utility yielded across each model (equally weighted)\n",
    "\n",
    "# Questions:\n",
    "\n",
    "# at what point is bayesian inference coming into play?\n",
    "# probability of what? what counts as an option for the agent\n",
    "\n",
    "# OPTIONS\n",
    "# aa\n",
    "# bb\n",
    "# ab\n",
    "# aa\n",
    "# bb\n",
    "# ab\n",
    "# aa\n",
    "# bb\n",
    "# ab\n",
    "# aaaa\n",
    "# bbbb\n",
    "# abab\n",
    "# aaaa\n",
    "# bbbb\n",
    "# abab\n",
    "# aaaa\n",
    "# bbbb\n",
    "# abab\n",
    "\n",
    "#CHOICE\n",
    "#aa\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
